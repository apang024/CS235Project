{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6beg4WqptUIJ"
   },
   "source": [
    "# Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "IXGSsrLwsfLU"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.stats import ttest_rel\n",
    "from sklearn.metrics import accuracy_score, make_scorer, precision_score, recall_score\n",
    "from sklearn.model_selection import cross_validate, train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Ye3TbByrD4K"
   },
   "source": [
    "# Importing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j7EqxaO3ponp"
   },
   "source": [
    "Enable displaying all columns when we print dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "PtV8yjYzpimT"
   },
   "outputs": [],
   "source": [
    "# Display all columns in output\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ct1vlkRTsQHb"
   },
   "source": [
    "Import training data. Change path if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "sJbZnPCisLEM"
   },
   "outputs": [],
   "source": [
    "TRAIN_PATH = './petfinder-adoption-prediction/train.csv'\n",
    "df = pd.read_csv(TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3oYCUajEa-JF"
   },
   "source": [
    "# General Pre-Processing\n",
    "\n",
    "These are the pre-processing steps that the whole team followed:\n",
    "- Drop unnecessary columns:\n",
    "  - `Name` was dropped because it has many missing values.\n",
    "  - `RescuerID` and `PetID` were dropped because they provide no meaning to our models.\n",
    "  - `Description` was dropped because we are not intending to do sentiment analysis on text for our project.\n",
    "- Drop rows with missing data.\n",
    "- Remove outliers for `Fee` and `Age` (numerical values). Outliers are those at least 5 standard deviations above the average for a column.\n",
    "\n",
    "I got help from Ashley, Ryan, and Charles for the code for these preprocessing steps. We all share these steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bt6CctAXcGvg",
    "outputId": "ed2e5abc-4f44-4985-9641-f590917cd04f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Type  Age  Breed1  Breed2  Gender  Color1  Color2  Color3  \\\n",
      "0         2    3     299       0       1       1       7       0   \n",
      "1         2    1     265       0       1       1       2       0   \n",
      "2         1    1     307       0       1       2       7       0   \n",
      "3         1    4     307       0       2       1       2       0   \n",
      "4         1    1     307       0       1       1       0       0   \n",
      "...     ...  ...     ...     ...     ...     ...     ...     ...   \n",
      "14988     2    2     266       0       3       1       0       0   \n",
      "14989     2   60     265     264       3       1       4       7   \n",
      "14990     2    2     265     266       3       5       6       7   \n",
      "14991     2    9     266       0       2       4       7       0   \n",
      "14992     1    1     307     307       1       2       0       0   \n",
      "\n",
      "       MaturitySize  FurLength  Vaccinated  Dewormed  Sterilized  Health  \\\n",
      "0                 1          1           2         2           2       1   \n",
      "1                 2          2           3         3           3       1   \n",
      "2                 2          2           1         1           2       1   \n",
      "3                 2          1           1         1           2       1   \n",
      "4                 2          1           2         2           2       1   \n",
      "...             ...        ...         ...       ...         ...     ...   \n",
      "14988             2          2           2         2           2       1   \n",
      "14989             2          2           1         1           1       1   \n",
      "14990             3          2           2         1           3       1   \n",
      "14991             1          1           1         1           1       1   \n",
      "14992             2          1           2         2           2       1   \n",
      "\n",
      "       Quantity  Fee  State  VideoAmt  PhotoAmt  AdoptionSpeed  \n",
      "0             1  100  41326         0       1.0              2  \n",
      "1             1    0  41401         0       2.0              0  \n",
      "2             1    0  41326         0       7.0              3  \n",
      "3             1  150  41401         0       8.0              2  \n",
      "4             1    0  41326         0       3.0              2  \n",
      "...         ...  ...    ...       ...       ...            ...  \n",
      "14988         4    0  41326         0       3.0              2  \n",
      "14989         2    0  41326         0       3.0              4  \n",
      "14990         5   30  41326         0       5.0              3  \n",
      "14991         1    0  41336         0       3.0              4  \n",
      "14992         1    0  41332         0       1.0              3  \n",
      "\n",
      "[14796 rows x 20 columns]\n"
     ]
    }
   ],
   "source": [
    "# Drop unnecessary columns\n",
    "columns_to_drop = ['Name', 'RescuerID', 'PetID', 'Description']\n",
    "df = df.drop(columns=columns_to_drop, axis=1)  # 1 refers to columns\n",
    "\n",
    "# Drop rows with missing data\n",
    "df = df.dropna()\n",
    "\n",
    "# Find z-scores for numerical columns\n",
    "z_scores_fee = np.abs(stats.zscore(df['Fee']))\n",
    "z_scores_age = np.abs(stats.zscore(df['Age']))\n",
    "\n",
    "# Find outliers based on threshold of 5 std. deviations\n",
    "outlier_rows_fee = z_scores_fee > 5\n",
    "outlier_rows_age = z_scores_age > 5\n",
    "combined_outlier_rows = outlier_rows_fee | outlier_rows_age\n",
    "\n",
    "# Remove outliers\n",
    "df = df[~combined_outlier_rows]\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jm9YPRFMnAYt"
   },
   "source": [
    "# Neural Network Pre-Processing\n",
    "\n",
    "This is pre-processing that is specific to my neural network model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lg-oi6z4tNJZ"
   },
   "source": [
    "Scale numerical attributes with `StandardScaler`. According to the `scikit-learn` [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html), `StandardScaler` should be used to scale numerical attributes for input to a machine learning estimator. This is to prevent features with large values from dominating the learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iIP0pcpjnL4s",
    "outputId": "50a03bf1-7f19-49d0-ec40-5502c6767544"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Age  Quantity       Fee  VideoAmt  PhotoAmt\n",
      "0     -0.430954 -0.393225  1.515422 -0.164405 -0.828418\n",
      "1     -0.558874 -0.393225 -0.308343 -0.164405 -0.542567\n",
      "2     -0.558874 -0.393225 -0.308343 -0.164405  0.886685\n",
      "3     -0.366994 -0.393225  2.427305 -0.164405  1.172536\n",
      "4     -0.558874 -0.393225 -0.308343 -0.164405 -0.256717\n",
      "...         ...       ...       ...       ...       ...\n",
      "14988 -0.494914  1.633300 -0.308343 -0.164405 -0.256717\n",
      "14989  3.214782  0.282284 -0.308343 -0.164405 -0.256717\n",
      "14990 -0.494914  2.308808  0.238787 -0.164405  0.314984\n",
      "14991 -0.047192 -0.393225 -0.308343 -0.164405 -0.256717\n",
      "14992 -0.558874 -0.393225 -0.308343 -0.164405 -0.828418\n",
      "\n",
      "[14796 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "features_to_scale = ['Age', 'Quantity', 'Fee', 'VideoAmt', 'PhotoAmt']\n",
    "df[features_to_scale] = scaler.fit_transform(df[features_to_scale])\n",
    "print(df[features_to_scale])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xQ7-j_fzE_e5"
   },
   "source": [
    "One-hot encode non-binary categorical features with `pd.get_dummies()`, documentation is [here](https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html). The below columns have values [1, 2, 3] for \"Yes\", \"No\", and \"Not Sure\". The one-hot encoding is done to prevent the neural network from assuming 1 < 2 < 3 for these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qEblyAbDET2g",
    "outputId": "efee044e-56d4-46dd-817a-d35e0bc23a80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Age  Breed1  Breed2  Color1  Color2  Color3  MaturitySize  \\\n",
      "0     -0.430954     299       0       1       7       0             1   \n",
      "1     -0.558874     265       0       1       2       0             2   \n",
      "2     -0.558874     307       0       2       7       0             2   \n",
      "3     -0.366994     307       0       1       2       0             2   \n",
      "4     -0.558874     307       0       1       0       0             2   \n",
      "...         ...     ...     ...     ...     ...     ...           ...   \n",
      "14988 -0.494914     266       0       1       0       0             2   \n",
      "14989  3.214782     265     264       1       4       7             2   \n",
      "14990 -0.494914     265     266       5       6       7             3   \n",
      "14991 -0.047192     266       0       4       7       0             1   \n",
      "14992 -0.558874     307     307       2       0       0             2   \n",
      "\n",
      "       FurLength  Health  Quantity       Fee  State  VideoAmt  PhotoAmt  \\\n",
      "0              1       1 -0.393225  1.515422  41326 -0.164405 -0.828418   \n",
      "1              2       1 -0.393225 -0.308343  41401 -0.164405 -0.542567   \n",
      "2              2       1 -0.393225 -0.308343  41326 -0.164405  0.886685   \n",
      "3              1       1 -0.393225  2.427305  41401 -0.164405  1.172536   \n",
      "4              1       1 -0.393225 -0.308343  41326 -0.164405 -0.256717   \n",
      "...          ...     ...       ...       ...    ...       ...       ...   \n",
      "14988          2       1  1.633300 -0.308343  41326 -0.164405 -0.256717   \n",
      "14989          2       1  0.282284 -0.308343  41326 -0.164405 -0.256717   \n",
      "14990          2       1  2.308808  0.238787  41326 -0.164405  0.314984   \n",
      "14991          1       1 -0.393225 -0.308343  41336 -0.164405 -0.256717   \n",
      "14992          1       1 -0.393225 -0.308343  41332 -0.164405 -0.828418   \n",
      "\n",
      "       AdoptionSpeed  Type_1  Type_2  Gender_1  Gender_2  Gender_3  \\\n",
      "0                  2       0       1         1         0         0   \n",
      "1                  0       0       1         1         0         0   \n",
      "2                  3       1       0         1         0         0   \n",
      "3                  2       1       0         0         1         0   \n",
      "4                  2       1       0         1         0         0   \n",
      "...              ...     ...     ...       ...       ...       ...   \n",
      "14988              2       0       1         0         0         1   \n",
      "14989              4       0       1         0         0         1   \n",
      "14990              3       0       1         0         0         1   \n",
      "14991              4       0       1         0         1         0   \n",
      "14992              3       1       0         1         0         0   \n",
      "\n",
      "       Vaccinated_1  Vaccinated_2  Vaccinated_3  Dewormed_1  Dewormed_2  \\\n",
      "0                 0             1             0           0           1   \n",
      "1                 0             0             1           0           0   \n",
      "2                 1             0             0           1           0   \n",
      "3                 1             0             0           1           0   \n",
      "4                 0             1             0           0           1   \n",
      "...             ...           ...           ...         ...         ...   \n",
      "14988             0             1             0           0           1   \n",
      "14989             1             0             0           1           0   \n",
      "14990             0             1             0           1           0   \n",
      "14991             1             0             0           1           0   \n",
      "14992             0             1             0           0           1   \n",
      "\n",
      "       Dewormed_3  Sterilized_1  Sterilized_2  Sterilized_3  \n",
      "0               0             0             1             0  \n",
      "1               1             0             0             1  \n",
      "2               0             0             1             0  \n",
      "3               0             0             1             0  \n",
      "4               0             0             1             0  \n",
      "...           ...           ...           ...           ...  \n",
      "14988           0             0             1             0  \n",
      "14989           0             1             0             0  \n",
      "14990           0             0             0             1  \n",
      "14991           0             1             0             0  \n",
      "14992           0             0             1             0  \n",
      "\n",
      "[14796 rows x 29 columns]\n"
     ]
    }
   ],
   "source": [
    "features_to_encode = ['Type', 'Gender', 'Vaccinated', 'Dewormed', 'Sterilized']\n",
    "df = pd.get_dummies(df, columns=features_to_encode, dtype=np.int8)  # Changed to int8 for manual implementation\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TWbxTrG5HUY1"
   },
   "source": [
    "Split `Breed1`, `Breed2`, and `State` features into N features each based on frequency of values.\n",
    "\n",
    "The breed columns have 306 unique values that have been label encoded, [1, 2, ..., 306]. We don't want the neural network to think there's a relationship between the numbers, like 100 < 101, but we can't one-hot encode because we will have too many columns.\n",
    "\n",
    "Instead, we create a column for each of the top 3 most common values, and another for \"other\".\n",
    "\n",
    "I used ChatGPT to help me with the dataframe manipulation in this code block, as I don't often use `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VJ3yHrwsIEKt",
    "outputId": "5b117a91-eec7-46b0-d1b3-3b4fcf969514"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 values for Breed1:\n",
      "Breed1\n",
      "307    5903\n",
      "266    3623\n",
      "265    1257\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 3 values for Breed2:\n",
      "Breed2\n",
      "0      10613\n",
      "307     1723\n",
      "266      597\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 3 values for State:\n",
      "State\n",
      "41326    8595\n",
      "41401    3800\n",
      "41327     830\n",
      "Name: count, dtype: int64\n",
      "\n",
      "            Age  Color1  Color2  Color3  MaturitySize  FurLength  Health  \\\n",
      "0     -0.430954       1       7       0             1          1       1   \n",
      "1     -0.558874       1       2       0             2          2       1   \n",
      "2     -0.558874       2       7       0             2          2       1   \n",
      "3     -0.366994       1       2       0             2          1       1   \n",
      "4     -0.558874       1       0       0             2          1       1   \n",
      "...         ...     ...     ...     ...           ...        ...     ...   \n",
      "14988 -0.494914       1       0       0             2          2       1   \n",
      "14989  3.214782       1       4       7             2          2       1   \n",
      "14990 -0.494914       5       6       7             3          2       1   \n",
      "14991 -0.047192       4       7       0             1          1       1   \n",
      "14992 -0.558874       2       0       0             2          1       1   \n",
      "\n",
      "       Quantity       Fee  VideoAmt  PhotoAmt  AdoptionSpeed  Type_1  Type_2  \\\n",
      "0     -0.393225  1.515422 -0.164405 -0.828418              2       0       1   \n",
      "1     -0.393225 -0.308343 -0.164405 -0.542567              0       0       1   \n",
      "2     -0.393225 -0.308343 -0.164405  0.886685              3       1       0   \n",
      "3     -0.393225  2.427305 -0.164405  1.172536              2       1       0   \n",
      "4     -0.393225 -0.308343 -0.164405 -0.256717              2       1       0   \n",
      "...         ...       ...       ...       ...            ...     ...     ...   \n",
      "14988  1.633300 -0.308343 -0.164405 -0.256717              2       0       1   \n",
      "14989  0.282284 -0.308343 -0.164405 -0.256717              4       0       1   \n",
      "14990  2.308808  0.238787 -0.164405  0.314984              3       0       1   \n",
      "14991 -0.393225 -0.308343 -0.164405 -0.256717              4       0       1   \n",
      "14992 -0.393225 -0.308343 -0.164405 -0.828418              3       1       0   \n",
      "\n",
      "       Gender_1  Gender_2  Gender_3  Vaccinated_1  Vaccinated_2  Vaccinated_3  \\\n",
      "0             1         0         0             0             1             0   \n",
      "1             1         0         0             0             0             1   \n",
      "2             1         0         0             1             0             0   \n",
      "3             0         1         0             1             0             0   \n",
      "4             1         0         0             0             1             0   \n",
      "...         ...       ...       ...           ...           ...           ...   \n",
      "14988         0         0         1             0             1             0   \n",
      "14989         0         0         1             1             0             0   \n",
      "14990         0         0         1             0             1             0   \n",
      "14991         0         1         0             1             0             0   \n",
      "14992         1         0         0             0             1             0   \n",
      "\n",
      "       Dewormed_1  Dewormed_2  Dewormed_3  Sterilized_1  Sterilized_2  \\\n",
      "0               0           1           0             0             1   \n",
      "1               0           0           1             0             0   \n",
      "2               1           0           0             0             1   \n",
      "3               1           0           0             0             1   \n",
      "4               0           1           0             0             1   \n",
      "...           ...         ...         ...           ...           ...   \n",
      "14988           0           1           0             0             1   \n",
      "14989           1           0           0             1             0   \n",
      "14990           1           0           0             0             0   \n",
      "14991           1           0           0             1             0   \n",
      "14992           0           1           0             0             1   \n",
      "\n",
      "       Sterilized_3  Breed1_-1  Breed1_265  Breed1_266  Breed1_307  Breed2_-1  \\\n",
      "0                 0          1           0           0           0          0   \n",
      "1                 1          0           1           0           0          0   \n",
      "2                 0          0           0           0           1          0   \n",
      "3                 0          0           0           0           1          0   \n",
      "4                 0          0           0           0           1          0   \n",
      "...             ...        ...         ...         ...         ...        ...   \n",
      "14988             0          0           0           1           0          0   \n",
      "14989             0          0           1           0           0          1   \n",
      "14990             1          0           1           0           0          0   \n",
      "14991             0          0           0           1           0          0   \n",
      "14992             0          0           0           0           1          0   \n",
      "\n",
      "       Breed2_0  Breed2_266  Breed2_307  State_-1  State_41326  State_41327  \\\n",
      "0             1           0           0         0            1            0   \n",
      "1             1           0           0         0            0            0   \n",
      "2             1           0           0         0            1            0   \n",
      "3             1           0           0         0            0            0   \n",
      "4             1           0           0         0            1            0   \n",
      "...         ...         ...         ...       ...          ...          ...   \n",
      "14988         1           0           0         0            1            0   \n",
      "14989         0           0           0         0            1            0   \n",
      "14990         0           1           0         0            1            0   \n",
      "14991         1           0           0         1            0            0   \n",
      "14992         0           0           1         1            0            0   \n",
      "\n",
      "       State_41401  \n",
      "0                0  \n",
      "1                1  \n",
      "2                0  \n",
      "3                1  \n",
      "4                0  \n",
      "...            ...  \n",
      "14988            0  \n",
      "14989            0  \n",
      "14990            0  \n",
      "14991            0  \n",
      "14992            0  \n",
      "\n",
      "[14796 rows x 38 columns]\n"
     ]
    }
   ],
   "source": [
    "n = 3\n",
    "features_to_split = ['Breed1', 'Breed2', 'State']\n",
    "\n",
    "for feature in features_to_split:\n",
    "    top_n_values = df[feature].value_counts().head(n)\n",
    "    print(f'Top {n} values for {feature}:\\n{top_n_values}\\n')\n",
    "\n",
    "    top_n_value_names = top_n_values.index\n",
    "    for index, row in df.iterrows():\n",
    "        # If value isn't top N frequency, replace with -1 (other)\n",
    "        if row[feature] not in top_n_value_names:\n",
    "            df.at[index, feature] = -1\n",
    "\n",
    "    df = pd.get_dummies(df, columns=[feature], dtype=np.int8)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xfM9swyu-ZWz"
   },
   "source": [
    "# Neural Network sklearn Implementation\n",
    "\n",
    "Set up neural network testing and training data. 85% is training, 10% is validation, and 5% is test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "u9Tckb6b-YUK"
   },
   "outputs": [],
   "source": [
    "# Separate X and y on input data\n",
    "target_feature = 'AdoptionSpeed'\n",
    "features = df.drop(columns=[target_feature])\n",
    "target = df[target_feature]\n",
    "\n",
    "# Split training data, 15% is for testing & validation\n",
    "x_train, x_test_val, y_train, y_test_val = train_test_split(\n",
    "    features, target, test_size=0.15, shuffle=True, stratify=target, random_state=42\n",
    ")\n",
    "\n",
    "# 5% of data is testing, 10% is validation\n",
    "x_test, x_val, y_test, y_val = train_test_split(\n",
    "    x_test_val, y_test_val, test_size=1 / 3, shuffle=True, stratify=y_test_val, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dCqwzXSyr1dn"
   },
   "source": [
    "Fit the neural network and assess accuracy on training, validation, and test sets.\n",
    "\n",
    "- `max-iter` was set to 500 to prevent the neural network from giving up due to too many iterations.\n",
    "\n",
    "- `hidden_layer_sizes` was determined after experimenting with different parameters with `GridSearchCV`. I ran a grid search on my local machine with a variety of layer sizes and found that `(38, 5)` gave the highest accuracy on test and validation data. 38 corresponds to the number of input features, and 5 corresponds to the number of categories we're predicting.\n",
    "\n",
    "- The `solver` of `adam` was kept because the `scikit-learn` [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html) recommends `adam` for datasets with thousands of samples or more. I considered using the solver `lbfgs`, but the neural network with `lbfgs` never converged.\n",
    "\n",
    "- `random_state` was set to 42 to make the predictions reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J6O0HXOor1do",
    "outputId": "29e4b45e-6e63-4e6a-e48d-3edbe38be2bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training neural network...\n",
      "\n",
      "Accuracy on training: 45.0%\n",
      "Accuracy on validation: 40.0%\n",
      "Accuracy on test: 38.7%\n",
      "\n",
      "Average weighted precision: 36.9%\n",
      "Average weighted recall: 38.7%\n",
      "\n",
      "Class 0: Precision = 0.0%, Recall = 0.0%\n",
      "Class 1: Precision = 30.9%, Recall = 29.5%\n",
      "Class 2: Precision = 33.7%, Recall = 43.1%\n",
      "Class 3: Precision = 37.7%, Recall = 13.3%\n",
      "Class 4: Precision = 47.4%, Recall = 64.9%\n",
      "\n",
      "Counts of each class in prediction: {1: 291, 2: 510, 3: 114, 4: 565}\n"
     ]
    }
   ],
   "source": [
    "classifier = MLPClassifier(max_iter=500, hidden_layer_sizes=(38, 5), random_state=42)\n",
    "\n",
    "# Fit to training data and make predictions on test data\n",
    "print('Training neural network...')\n",
    "classifier.fit(x_train, y_train)\n",
    "test_predictions = classifier.predict(x_test)\n",
    "\n",
    "accuracy = classifier.score(x_train, y_train)\n",
    "print(f'\\nAccuracy on training: {round(accuracy, 3) * 100}%')\n",
    "\n",
    "accuracy = classifier.score(x_val, y_val)\n",
    "print(f'Accuracy on validation: {round(accuracy, 3) * 100}%')\n",
    "\n",
    "accuracy = classifier.score(x_test, y_test)\n",
    "print(f'Accuracy on test: {round(accuracy, 3) * 100}%\\n')\n",
    "\n",
    "# zero_divison=0 returns 0 precision/recall if no positive samples for a class\n",
    "avg_precision = precision_score(y_test, test_predictions, average='weighted', zero_division=0)\n",
    "avg_recall = recall_score(y_test, test_predictions, average='weighted', zero_division=0)\n",
    "precision_per_class = precision_score(y_test, test_predictions, average=None, zero_division=0)\n",
    "recall_per_class = recall_score(y_test, test_predictions, average=None, zero_division=0)\n",
    "\n",
    "print(f'Average weighted precision: {round(avg_precision, 3) * 100}%')\n",
    "print(f'Average weighted recall: {round(avg_recall, 3) * 100}%\\n')\n",
    "for class_label, precision, recall in zip(range(len(precision_per_class)), precision_per_class, recall_per_class):\n",
    "    print(f'Class {class_label}: Precision = {round(precision, 3) * 100}%, Recall = {round(recall, 3) * 100}%')\n",
    "print()\n",
    "\n",
    "unique_values, counts = np.unique(test_predictions, return_counts=True)\n",
    "print(f'Counts of each class in prediction: {dict(zip(unique_values, counts))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6pd8moBOp5NF"
   },
   "source": [
    "Perform k-fold cross validation for modified parameters.\n",
    "\n",
    "Output cross-validation accuracy, precision, and recall (weighted averages)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "8-rlkjjwp3cH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10-fold validation accuracy mean: 37.8%\n",
      "10-fold validation precision mean: 35.9%\n",
      "10-fold validation recall mean: 37.8%\n"
     ]
    }
   ],
   "source": [
    "num_folds = 10\n",
    "scoring_metrics = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'precision': make_scorer(precision_score, average='weighted', zero_division=0),\n",
    "    'recall': make_scorer(recall_score, average='weighted', zero_division=0),\n",
    "}\n",
    "scores_modified = cross_validate(classifier, x_train, y_train, cv=num_folds, scoring=scoring_metrics)\n",
    "\n",
    "print(f'\\n{num_folds}-fold validation accuracy mean: {round(scores_modified[\"test_accuracy\"].mean(), 3) * 100}%')\n",
    "print(f'{num_folds}-fold validation precision mean: {round(scores_modified[\"test_precision\"].mean(), 3) * 100}%')\n",
    "print(f'{num_folds}-fold validation recall mean: {round(scores_modified[\"test_recall\"].mean(), 3) * 100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9leSiYICr6pz"
   },
   "source": [
    "Repeating the same experiment with a default instance of `MLPClassifier`. `max_iter` was changed to 500 because the default value would not converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7DsJSENYr6Ug",
    "outputId": "fad9a3e8-c854-4830-cce2-e73554368740"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training neural network...\n",
      "\n",
      "Accuracy on training: 51.2%\n",
      "Accuracy on validation: 40.699999999999996%\n",
      "Accuracy on test: 37.1%\n",
      "\n",
      "Average weighted precision: 35.3%\n",
      "Average weighted recall: 37.1%\n",
      "\n",
      "Class 0: Precision = 0.0%, Recall = 0.0%\n",
      "Class 1: Precision = 30.5%, Recall = 23.9%\n",
      "Class 2: Precision = 33.0%, Recall = 38.6%\n",
      "Class 3: Precision = 35.9%, Recall = 21.4%\n",
      "Class 4: Precision = 44.0%, Recall = 61.3%\n",
      "\n",
      "Counts of each class in prediction: {0: 8, 1: 239, 2: 466, 3: 192, 4: 575}\n"
     ]
    }
   ],
   "source": [
    "classifier = MLPClassifier(max_iter=500, random_state=42)\n",
    "\n",
    "# Fit to training data and make predictions on test data\n",
    "print('Training neural network...')\n",
    "classifier.fit(x_train, y_train)\n",
    "test_predictions = classifier.predict(x_test)\n",
    "\n",
    "accuracy = classifier.score(x_train, y_train)\n",
    "print(f'\\nAccuracy on training: {round(accuracy, 3) * 100}%')\n",
    "\n",
    "accuracy = classifier.score(x_val, y_val)\n",
    "print(f'Accuracy on validation: {round(accuracy, 3) * 100}%')\n",
    "\n",
    "accuracy = classifier.score(x_test, y_test)\n",
    "print(f'Accuracy on test: {round(accuracy, 3) * 100}%\\n')\n",
    "\n",
    "# zero_divison=0 returns 0 precision/recall if no positive samples for a class\n",
    "avg_precision = precision_score(y_test, test_predictions, average='weighted', zero_division=0)\n",
    "avg_recall = recall_score(y_test, test_predictions, average='weighted', zero_division=0)\n",
    "precision_per_class = precision_score(y_test, test_predictions, average=None, zero_division=0)\n",
    "recall_per_class = recall_score(y_test, test_predictions, average=None, zero_division=0)\n",
    "\n",
    "print(f'Average weighted precision: {round(avg_precision, 3) * 100}%')\n",
    "print(f'Average weighted recall: {round(avg_recall, 3) * 100}%\\n')\n",
    "for class_label, precision, recall in zip(range(len(precision_per_class)), precision_per_class, recall_per_class):\n",
    "    print(f'Class {class_label}: Precision = {round(precision, 3) * 100}%, Recall = {round(recall, 3) * 100}%')\n",
    "print()\n",
    "\n",
    "unique_values, counts = np.unique(test_predictions, return_counts=True)\n",
    "print(f'Counts of each class in prediction: {dict(zip(unique_values, counts))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hhlr0T7l3eWi"
   },
   "source": [
    "Perform k-fold cross validation for default parameters.\n",
    "\n",
    "Output cross-validation accuracy, precision, and recall (weighted averages)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "QdKHouuR3WrL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10-fold validation accuracy mean: 36.199999999999996%\n",
      "10-fold validation precision mean: 34.9%\n",
      "10-fold validation recall mean: 36.199999999999996%\n"
     ]
    }
   ],
   "source": [
    "num_folds = 10\n",
    "scoring_metrics = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'precision': make_scorer(precision_score, average='weighted', zero_division=0),\n",
    "    'recall': make_scorer(recall_score, average='weighted', zero_division=0),\n",
    "}\n",
    "scores_default = cross_validate(classifier, x_train, y_train, cv=num_folds, scoring=scoring_metrics)\n",
    "\n",
    "print(f'\\n{num_folds}-fold validation accuracy mean: {round(scores_default[\"test_accuracy\"].mean(), 3) * 100}%')\n",
    "print(f'{num_folds}-fold validation precision mean: {round(scores_default[\"test_precision\"].mean(), 3) * 100}%')\n",
    "print(f'{num_folds}-fold validation recall mean: {round(scores_default[\"test_recall\"].mean(), 3) * 100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V3gAEBTTzxOp"
   },
   "source": [
    "Find p-values for accuracy, precision, and recall from cross-validations.\n",
    "\n",
    "Got help from Ashley for this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fpMPrZYRq-Ws",
    "outputId": "d4625d91-557b-4702-ae6c-c9226ae692d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy p-val: 0.0\n",
      "Precision p-val: 0.015\n",
      "Recall p-val: 0.0\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.05  # Significance level\n",
    "\n",
    "ttest_accuracy, p_val_accuracy = ttest_rel(scores_modified['test_accuracy'], scores_default['test_accuracy'])\n",
    "ttest_precision, p_val_precision = ttest_rel(scores_modified['test_precision'], scores_default['test_precision'])\n",
    "ttest_recall, p_val_recall = ttest_rel(scores_modified['test_recall'], scores_default['test_recall'])\n",
    "\n",
    "print(f'\\nAccuracy p-val: {round(p_val_accuracy, 3)}')\n",
    "print(f'Precision p-val: {round(p_val_precision, 3)}')\n",
    "print(f'Recall p-val: {round(p_val_recall, 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Manual Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the classes used for my manual implementation of sklearn's neural network:\n",
    "- `SoftmaxLogLoss`: Combined Softmax activation layer and Log Loss function.\n",
    "- `LogLoss`: Loss function for model, finds log loss for each sample and average loss.\n",
    "- `Softmax`: Softmax activation layer for the model's output. Gives class probabilities for samples.\n",
    "- `ReLU`: Rectified linear unit activation layer. Used as the activation function for hidden layers.\n",
    "- `Layer`: Implements a hidden layer with neurons; each neuron has weights and a bias.\n",
    "- `OptimizerSGD`: A simple implementation of stochastic gradient descent, optimizes the model's parameters.\n",
    "- `NeuralNet`: Encapsulates all of the above classes with helper functions to mimimc sklearn's MLP class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Citations:\n",
    "\n",
    "1) \"Neural Networks from Scratch in Python\" by Harrison Kinsley, Daniel Kukieła (2020), https://nnfs.io/\n",
    "\n",
    "This textbook and the corresponding YouTube series were a great help in this project.\n",
    "With permission from Manish, I used this book's solution for the .backward() methods (backpropagation), as\n",
    "implementing these methods myself for each layer was beyond my understanding (especially the Softmax and \n",
    "Log Loss layers). I used chapters 8 through 10 in the book.\n",
    "\n",
    "2) These playlists helped me understand how to implement the forward passes:\n",
    "\n",
    "\"Neural Networks from Scratch in Python\" by sentdex:\n",
    "https://youtube.com/playlist?list=PLQVvvaa0QuDcjD5BAw2DxE6OF2tius3V3&feature=shared\n",
    "\n",
    "\"Neural networks\" by 3Blue1Brown:\n",
    "https://youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&feature=shared\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class SoftmaxLogLoss:\n",
    "    \"\"\"Combined Softmax activation function and Log Loss function.\n",
    "\n",
    "    This is done to calculate the combined gradient of the loss function\n",
    "    and the Softmax activation function. This ends up being much faster\n",
    "    during backpropagation compared to separately calculating the gradient\n",
    "    of the activation and loss functions.\n",
    "\n",
    "    Attributes:\n",
    "        activation (Softmax): Softmax activation layer, finds class probabilities.\n",
    "        loss (LogLoss): Log loss layer, finds cross entropy loss for samples.\n",
    "        dinputs (np.ndarray): Gradient of loss function w.r.t. this layer's inputs.\n",
    "            - Used for backpropagation during training.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.activation = Softmax()\n",
    "        self.loss = LogLoss()\n",
    "\n",
    "    def forward(self, inputs: np.ndarray, target_vals: np.ndarray):\n",
    "        \"\"\"Execute forward pass for Softmax and Loss layers.\"\"\"\n",
    "        self.activation.forward(inputs)  # Output layer's Softmax activation\n",
    "        self.loss.forward(self.activation.output, target_vals)  # Calculate loss for each sample\n",
    "\n",
    "    def backward(self, dvalues: np.ndarray, target_vals: np.ndarray):\n",
    "        \"\"\"Find the gradient of the model's loss function w.r.t. this layer's inputs.\"\"\"\n",
    "        num_samples = len(dvalues)\n",
    "\n",
    "        # If target labels are one-hot encoded, convert to [0, 1, 2, ...]\n",
    "        if target_vals.ndim > 1:\n",
    "            # Get index of max value in each row\n",
    "            target_vals = np.argmax(target_vals, axis=1)\n",
    "\n",
    "        self.dinputs = dvalues.copy()  # Don't modify input gradient\n",
    "        self.dinputs[range(num_samples), target_vals] -= 1  # Calculate gradient\n",
    "        self.dinputs = self.dinputs / num_samples  # Normalize gradient\n",
    "\n",
    "\n",
    "class LogLoss:\n",
    "    \"\"\"Log loss (cross entropy) function for the neural network.\n",
    "\n",
    "    This class calculates log loss for each sample between the model's predicted\n",
    "    probabilities and target values. The class also calculates average loss\n",
    "    across all samples.\n",
    "\n",
    "    Input to this layer is the output of the Softmax layer, which is a matrix of\n",
    "    predicted class probabilities for each sample.\n",
    "\n",
    "    Attributes:\n",
    "        log_losses (np.ndarray): Log losses for each sample.\n",
    "        avg_loss (float): Average log loss across all samples.\n",
    "        dinputs (np.ndarray): Gradient of loss function w.r.t. this layer's inputs.\n",
    "            - Used for backpropagation during training.\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, predictions: np.ndarray, target_vals: np.ndarray):\n",
    "        \"\"\"Find log loss values for each sample, and average log loss across all samples.\"\"\"\n",
    "        # Target vals are a 1D vector of scalars, e.g. [0, 1, 2]\n",
    "        # Treat each value as an index\n",
    "        if target_vals.ndim == 1:\n",
    "            target_probs = predictions[range(len(predictions)), target_vals]\n",
    "\n",
    "        # Target values are one-hot encoded\n",
    "        elif target_vals > 1:\n",
    "            target_probs = predictions * target_vals  # Everything 0 except target index\n",
    "            target_probs = np.sum(target_probs, axis=1)  # Isolates non-zero value\n",
    "\n",
    "        # Ensure softmax vals are in range (0, 1) excluding 0 and 1\n",
    "        # Prevents division by 0 when we compute average loss\n",
    "        # -ln(1) = 0 and -ln(0) = inf\n",
    "        probs_clipped = np.clip(target_probs, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Compute loss for each sample with negative log (log-loss)\n",
    "        # Perfect prediction has all ones\n",
    "        self.log_losses = -np.log(probs_clipped)\n",
    "\n",
    "        # Compute average loss across all samples\n",
    "        self.avg_loss = np.mean(self.log_losses)\n",
    "\n",
    "    def backward(self, dvalues, y_true):\n",
    "        \"\"\"Find the gradient of the model's loss function w.r.t. this layer's inputs.\"\"\"\n",
    "        num_samples = len(dvalues)\n",
    "        num_labels = len(dvalues[0])\n",
    "\n",
    "        # If labels are a 1D vector, convert to one-hot encoding\n",
    "        if y_true.ndim == 1:\n",
    "            y_true = np.eye(num_labels)[y_true]\n",
    "\n",
    "        self.dinputs = -y_true / dvalues  # Calculate gradient\n",
    "        self.dinputs = self.dinputs / num_samples  # Normalize gradient\n",
    "\n",
    "\n",
    "class Softmax:\n",
    "    \"\"\"Calculates a probability distribution of class predictions for each sample.\n",
    "\n",
    "    The output of this class is provided to the log loss function at the model's output.\n",
    "\n",
    "    Attributes:\n",
    "        output (np.ndarray): Probability distribution for classes for every sample.\n",
    "        inputs (np.ndarray): Inputs to the layer, kept for backpropagation.\n",
    "        dinputs (np.ndarray): Gradient of loss function w.r.t. this layer's inputs.\n",
    "            - Used for backpropagation during training.\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, inputs: np.ndarray):\n",
    "        \"\"\"Finds probabilities that each sample belongs to each class.\"\"\"\n",
    "        self.inputs = inputs  # Keep inputs for backpropagation\n",
    "\n",
    "        # Find max of all features for each sample\n",
    "        # Result has same number of rows as the input, one per sample\n",
    "        max_of_each_row = np.max(inputs, axis=1, keepdims=True)\n",
    "\n",
    "        # Subtract max value in each row, prevents overflow\n",
    "        exp_inputs = np.exp(inputs - max_of_each_row)\n",
    "\n",
    "        # Find sum of all features for each sample\n",
    "        # Result has same number of rows as the input, one per sample\n",
    "        sum_of_each_row = np.sum(exp_inputs, axis=1, keepdims=True)\n",
    "\n",
    "        # Divide each row by its corresponding sum\n",
    "        probabilities = exp_inputs / sum_of_each_row\n",
    "\n",
    "        self.output = probabilities\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        \"\"\"Find the gradient of the model's loss function w.r.t. this layer's inputs.\"\"\"\n",
    "        self.dinputs = np.empty_like(dvalues)  # Uninitialized array\n",
    "\n",
    "        # Calculate Jacobian matrix of output and sample-wise gradients\n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
    "            single_output = single_output.reshape(-1, 1)\n",
    "            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)\n",
    "\n",
    "\n",
    "class ReLU:\n",
    "    \"\"\"The Rectified Linear Unit (ReLU) activation function.\n",
    "\n",
    "    Applies the ReLU activation function to a layer's output.\n",
    "    Used to make a neural network's prediction nonlinear.\n",
    "\n",
    "    Attributes:\n",
    "        output (np.ndarray): The result of applying the ReLU function to each element in a layer's output.\n",
    "        inputs (np.ndarray): Inputs to the layer, kept for backpropagation.\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, inputs: np.ndarray):\n",
    "        \"\"\"Applies the ReLU function element-wise: the maximum of 0 and the element.\"\"\"\n",
    "        self.output = np.maximum(0, inputs)\n",
    "        self.inputs = inputs  # Keep inputs for for backpropagation\n",
    "\n",
    "    def backward(self, dvalues: np.ndarray):\n",
    "        \"\"\"Find the gradient of the model's loss function w.r.t. this layer's inputs.\"\"\"\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "\n",
    "\n",
    "class Layer:\n",
    "    \"\"\"One hidden layer in a neural network.\n",
    "\n",
    "    Attributes:\n",
    "        weights (np.ndarray): An (n_inputs, n_neurons) matrix of feature weights per neuron.\n",
    "            - Weights are shape (n_inputs, n_neurons) to enable the dot product.\n",
    "            - Each column has the weights for one neuron, a row has the weights for one feature.\n",
    "            - Multiplying the matrix by a constant keeps the weights small.\n",
    "        biases (np.ndarray): A (1, n_neurons) vector of biases, one for each neuron.\n",
    "        output (np.ndarray): A weighted sum + bias for each neuron in the layer.\n",
    "        inputs (np.ndarray): Inputs to the layer, kept for backpropagation.\n",
    "        dweights (np.ndarray): Gradient of loss function w.r.t. this layer's weights.\n",
    "        dbiases (np.ndarray): Gradient of loss function w.r.t. this layer's biases.\n",
    "        dinputs (np.ndarray): Gradient of loss function w.r.t. this layer's inputs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_inputs: int, n_neurons: int):\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    def forward(self, inputs: np.ndarray):\n",
    "        \"\"\"Computes a weighted sum + bias for each neuron (the dot product).\"\"\"\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    def backward(self, dvalues: np.ndarray):\n",
    "        \"\"\"Find the gradient of the model's loss function w.r.t. this layer's weights, biases, & inputs.\"\"\"\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "\n",
    "class OptimizerSGD:\n",
    "    \"\"\"A stochastic gradient descent optimizer for the model's parameters.\n",
    "\n",
    "    Adjusts the weights and biases of a layer using the negative gradient\n",
    "    of the loss function w.r.t each weight and bias, scaled by a learning rate.\n",
    "\n",
    "    Attributes:\n",
    "        learning_rate (float): A coefficient that controls how large our \"steps\" are.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, learning_rate: float = 0.1):\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def update_params(self, layer: Layer):\n",
    "        \"\"\"Update the weights and biases of a layer using the gradient of the loss function.\"\"\"\n",
    "        layer.weights += -(self.learning_rate * layer.dweights)\n",
    "        layer.biases += -(self.learning_rate * layer.dbiases)\n",
    "\n",
    "\n",
    "class NeuralNet:\n",
    "    \"\"\"A neural network that tries to minimize a log loss function using stochastic gradient descent.\n",
    "\n",
    "    Models an input layer, two hidden layers, and an output layer.\n",
    "    Hidden layers use the ReLU activation function, output uses Softmax.\n",
    "    Loss is calculated using log loss (cross entropy).\n",
    "    Loss is minimized using stochastic gradient descent.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_num_features: int,\n",
    "        output_num_classes: int,\n",
    "        hidden_layer_sizes: tuple[int, int],\n",
    "        max_iter: int = 200,\n",
    "        learning_rate: float = 0.1,\n",
    "    ):\n",
    "        self.layer1 = Layer(input_num_features, hidden_layer_sizes[0])  # Input and first hidden layer\n",
    "        self.activation1 = ReLU()\n",
    "        self.layer2 = Layer(hidden_layer_sizes[0], hidden_layer_sizes[1])  # Second hidden layer\n",
    "        self.activation2 = ReLU()\n",
    "        self.layer3 = Layer(hidden_layer_sizes[1], output_num_classes)  # Output layer\n",
    "        self.loss_activation = SoftmaxLogLoss()\n",
    "        self.sgd_optimizer = OptimizerSGD(learning_rate=learning_rate)\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "    def _get_numpy_array(self, arr: np.ndarray | pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"Convert a Pandas dataframe/series to a Numpy array if the input is a dataframe/series.\"\"\"\n",
    "        arr_np = arr\n",
    "        if isinstance(arr, pd.DataFrame) or isinstance(arr, pd.Series):\n",
    "            arr_np = arr.to_numpy()\n",
    "        return arr_np\n",
    "\n",
    "    def _get_class_predictions(self) -> np.ndarray:\n",
    "        \"\"\"Return a list of class predictions for each sample, e.g. [0, 1, ..., 4].\"\"\"\n",
    "        # Probabilities that each sample belongs to each class, from Softmax output\n",
    "        class_probabilities = self.loss_activation.activation.output\n",
    "\n",
    "        # Index of the highest probability class per sample\n",
    "        # Represents what class was predicted for each sample, e.g. [0, 1, .., 4]\n",
    "        class_predictions = np.argmax(class_probabilities, axis=1)\n",
    "\n",
    "        return class_predictions\n",
    "\n",
    "    def fit(self, x_train: np.ndarray | pd.DataFrame, y_train: np.ndarray | pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"Train the model on x, y data using S.G.D over max_iter epochs, return class predictions.\"\"\"\n",
    "        x_train_np = self._get_numpy_array(x_train)\n",
    "        y_train_np = self._get_numpy_array(y_train)\n",
    "\n",
    "        percent_iterations = self.max_iter / 5\n",
    "\n",
    "        # Train max_iter number of times\n",
    "        for epoch in range(self.max_iter):\n",
    "            # Forward pass\n",
    "            self.layer1.forward(x_train_np)\n",
    "            self.activation1.forward(self.layer1.output)\n",
    "            self.layer2.forward(self.activation1.output)\n",
    "            self.activation2.forward(self.layer2.output)\n",
    "            self.layer3.forward(self.activation2.output)\n",
    "            self.loss_activation.forward(self.layer3.output, y_train_np)\n",
    "\n",
    "            if epoch % percent_iterations == 0:\n",
    "                print(f'Epoch: {epoch}; Loss: {self.loss_activation.loss.avg_loss}')\n",
    "\n",
    "            # Backward pass\n",
    "            self.loss_activation.backward(self.loss_activation.activation.output, y_train_np)\n",
    "            self.layer3.backward(self.loss_activation.dinputs)\n",
    "            self.activation2.backward(self.layer3.dinputs)\n",
    "            self.layer2.backward(self.activation2.dinputs)\n",
    "            self.activation1.backward(self.layer2.dinputs)\n",
    "            self.layer1.backward(self.activation1.dinputs)\n",
    "\n",
    "            # Tune model parameters\n",
    "            self.sgd_optimizer.update_params(self.layer1)\n",
    "            self.sgd_optimizer.update_params(self.layer2)\n",
    "            self.sgd_optimizer.update_params(self.layer3)\n",
    "\n",
    "        return self._get_class_predictions()\n",
    "\n",
    "    def predict(self, x_test: np.ndarray | pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"Predicts classes for test data, returns the class predictions.\"\"\"\n",
    "        x_test_np = self._get_numpy_array(x_test)\n",
    "\n",
    "        # Forward pass\n",
    "        self.layer1.forward(x_test_np)\n",
    "        self.activation1.forward(self.layer1.output)\n",
    "        self.layer2.forward(self.activation1.output)\n",
    "        self.activation2.forward(self.layer2.output)\n",
    "        self.layer3.forward(self.activation2.output)\n",
    "        self.loss_activation.activation.forward(self.layer3.output)  # Only probabilities, no loss\n",
    "\n",
    "        return self._get_class_predictions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to perform cross validation on our \"from scratch\" implementation.\n",
    "\n",
    "`cross_validate_manual()` comes from Ashley Pang."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code comes from Ashley (apang024)\n",
    "def cross_validate_manual(model: NeuralNet, x, y, cv: int, scoring: dict, random_seed=None) -> dict:\n",
    "    \"\"\"Performs cross-validation on our custom model objects, returns results.\"\"\"\n",
    "    accuracy_list = []\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    # Per split\n",
    "    for _ in range(cv):\n",
    "        x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=1 / cv, random_state=random_seed)\n",
    "        model.fit(x_train, y_train)\n",
    "        predictions = model.predict(x_val)\n",
    "\n",
    "        for metric in scoring:\n",
    "            if metric == 'accuracy':\n",
    "                _accuracy_score = accuracy_score(y_val, predictions)\n",
    "                accuracy_list.append(_accuracy_score)\n",
    "            elif metric == 'precision':\n",
    "                _precision_score = precision_score(y_val, predictions, average='weighted', zero_division=0)\n",
    "                precision_list.append(_precision_score)\n",
    "            elif metric == 'recall':\n",
    "                _recall_score = recall_score(y_val, predictions, average='weighted', zero_division=0)\n",
    "                recall_list.append(_recall_score)\n",
    "\n",
    "    accuracy_array = np.array(accuracy_list)\n",
    "    precision_array = np.array(precision_list)\n",
    "    recall_array = np.array(recall_list)\n",
    "\n",
    "    result_dictionary = {\n",
    "        'test_accuracy': accuracy_array,\n",
    "        'test_precision': precision_array,\n",
    "        'test_recall': recall_array,\n",
    "    }\n",
    "\n",
    "    return result_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the neural network and assess accuracy on training and test sets. Use the same data that the sklearn model used.\n",
    "\n",
    "Here we use the same parameters as our optimized sklearn model:\n",
    "\n",
    "- `max-iter` is 500, though my manual implementation performs very poorly because of this.\n",
    "\n",
    "- `hidden_layer_sizes` is `(37, 5)` to represent 37 input features and 5 output classes. This was mistakenly set to `(38, 5)` for the sklearn version because I thought there were 38 input features.\n",
    "\n",
    "- `learning_rate` is `0.001` to match the default value used in our [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html) model.\n",
    "\n",
    "- `input_num_features` and `output_num_classes` are explicitly set to `37` and `5` for my implementation to work correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training neural network...\n",
      "Epoch: 0; Loss: 1.6094459496021978\n",
      "Epoch: 100; Loss: 1.6053942918620308\n",
      "Epoch: 200; Loss: 1.6014999618200427\n",
      "Epoch: 300; Loss: 1.5977553265778615\n",
      "Epoch: 400; Loss: 1.5941531917133942\n",
      "\n",
      "Accuracy on training: 28.000000000000004%\n",
      "Accuracy on validation: 37.1%\n",
      "Accuracy on test: 27.900000000000002%\n",
      "\n",
      "Average weighted precision: 7.8%\n",
      "Average weighted recall: 27.900000000000002%\n",
      "\n",
      "Class 0: Precision = 0.0%, Recall = 0.0%\n",
      "Class 1: Precision = 0.0%, Recall = 0.0%\n",
      "Class 2: Precision = 0.0%, Recall = 0.0%\n",
      "Class 3: Precision = 0.0%, Recall = 0.0%\n",
      "Class 4: Precision = 27.900000000000002%, Recall = 100.0%\n",
      "\n",
      "Training predictions: [4 4 4 ... 4 4 4]\n",
      "Counts of each class in prediction: {4: 12576}\n",
      "Test predictions: [4 4 4 ... 4 4 4]\n",
      "Counts of each class in prediction: {4: 1480}\n"
     ]
    }
   ],
   "source": [
    "mlp = NeuralNet(\n",
    "    input_num_features=37, output_num_classes=5, hidden_layer_sizes=(37, 5), max_iter=500, learning_rate=0.001\n",
    ")\n",
    "\n",
    "# Fit to training data and make predictions on test data\n",
    "print('Training neural network...')\n",
    "train_predictions = mlp.fit(x_train, y_train)\n",
    "val_predictions = mlp.predict(x_val)\n",
    "test_predictions = mlp.predict(x_test)\n",
    "\n",
    "train_accuracy = accuracy_score(train_predictions, y_train.to_numpy())\n",
    "print(f'\\nAccuracy on training: {round(train_accuracy, 3) * 100}%')\n",
    "\n",
    "val_accuracy = accuracy_score(val_predictions, y_val.to_numpy())\n",
    "print(f'Accuracy on validation: {round(accuracy, 3) * 100}%')\n",
    "\n",
    "test_accuracy = accuracy_score(test_predictions, y_test.to_numpy())\n",
    "print(f'Accuracy on test: {round(test_accuracy, 3) * 100}%\\n')\n",
    "\n",
    "# zero_divison=0 returns 0 precision/recall if no positive samples for a class\n",
    "avg_precision = precision_score(y_test, test_predictions, average='weighted', zero_division=0)\n",
    "avg_recall = recall_score(y_test, test_predictions, average='weighted', zero_division=0)\n",
    "precision_per_class = precision_score(y_test, test_predictions, average=None, zero_division=0)\n",
    "recall_per_class = recall_score(y_test, test_predictions, average=None, zero_division=0)\n",
    "\n",
    "print(f'Average weighted precision: {round(avg_precision, 3) * 100}%')\n",
    "print(f'Average weighted recall: {round(avg_recall, 3) * 100}%\\n')\n",
    "for class_label, precision, recall in zip(range(len(precision_per_class)), precision_per_class, recall_per_class):\n",
    "    print(f'Class {class_label}: Precision = {round(precision, 3) * 100}%, Recall = {round(recall, 3) * 100}%')\n",
    "print()\n",
    "\n",
    "unique_values, counts = np.unique(train_predictions, return_counts=True)\n",
    "print(f'Training predictions: {train_predictions}')\n",
    "print(f'Counts of each class in prediction: {dict(zip(unique_values, counts))}')\n",
    "\n",
    "unique_values, counts = np.unique(test_predictions, return_counts=True)\n",
    "print(f'Test predictions: {test_predictions}')\n",
    "print(f'Counts of each class in prediction: {dict(zip(unique_values, counts))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform k-fold cross validation for manual implementation with modified parameters.\n",
    "\n",
    "Output cross-validation accuracy, precision, and recall (weighted averages)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing cross-validation with 10 folds...\n",
      "Epoch: 0; Loss: 1.5905632431809045\n",
      "Epoch: 100; Loss: 1.587177316206561\n",
      "Epoch: 200; Loss: 1.5839165707875962\n",
      "Epoch: 300; Loss: 1.5807752211115402\n",
      "Epoch: 400; Loss: 1.5777477981179\n",
      "Epoch: 0; Loss: 1.5750861248949932\n",
      "Epoch: 100; Loss: 1.572317279219323\n",
      "Epoch: 200; Loss: 1.569645885846085\n",
      "Epoch: 300; Loss: 1.567067628601712\n",
      "Epoch: 400; Loss: 1.5645784177648425\n",
      "Epoch: 0; Loss: 1.5623489296678466\n",
      "Epoch: 100; Loss: 1.5600460457837537\n",
      "Epoch: 200; Loss: 1.5578206083727568\n",
      "Epoch: 300; Loss: 1.5556693362482588\n",
      "Epoch: 400; Loss: 1.5535891125246155\n",
      "Epoch: 0; Loss: 1.5513656411223282\n",
      "Epoch: 100; Loss: 1.5494005866244824\n",
      "Epoch: 200; Loss: 1.5474985374930537\n",
      "Epoch: 300; Loss: 1.5456569228082795\n",
      "Epoch: 400; Loss: 1.543873295864539\n",
      "Epoch: 0; Loss: 1.542565326840721\n",
      "Epoch: 100; Loss: 1.540915629668147\n",
      "Epoch: 200; Loss: 1.5393163737089386\n",
      "Epoch: 300; Loss: 1.5377655831786368\n",
      "Epoch: 400; Loss: 1.5362613744731382\n",
      "Epoch: 0; Loss: 1.534543015384287\n",
      "Epoch: 100; Loss: 1.5331113988137575\n",
      "Epoch: 200; Loss: 1.5317215160826692\n",
      "Epoch: 300; Loss: 1.5303717942345283\n",
      "Epoch: 400; Loss: 1.5290607309432658\n",
      "Epoch: 0; Loss: 1.5269744564608705\n",
      "Epoch: 100; Loss: 1.525706237631254\n",
      "Epoch: 200; Loss: 1.5244738591016487\n",
      "Epoch: 300; Loss: 1.5232760144608632\n",
      "Epoch: 400; Loss: 1.5221114541863847\n",
      "Epoch: 0; Loss: 1.5223180894891934\n",
      "Epoch: 100; Loss: 1.5212614807284597\n",
      "Epoch: 200; Loss: 1.5202332894851522\n",
      "Epoch: 300; Loss: 1.5192325244734366\n",
      "Epoch: 400; Loss: 1.5182582357699004\n",
      "Epoch: 0; Loss: 1.516741209689575\n",
      "Epoch: 100; Loss: 1.5158021932240155\n",
      "Epoch: 200; Loss: 1.5148876461554917\n",
      "Epoch: 300; Loss: 1.5139967401513779\n",
      "Epoch: 400; Loss: 1.5131286812532592\n",
      "Epoch: 0; Loss: 1.511670160940863\n",
      "Epoch: 100; Loss: 1.51082735532422\n",
      "Epoch: 200; Loss: 1.5100055523133338\n",
      "Epoch: 300; Loss: 1.5092040659681685\n",
      "Epoch: 400; Loss: 1.5084222359231345\n",
      "\n",
      "10-fold validation accuracy mean: 27.700000000000003%\n",
      "10-fold validation precision mean: 7.7%\n",
      "10-fold validation recall mean: 27.700000000000003%\n"
     ]
    }
   ],
   "source": [
    "# Get cross-validation metrics for the manual implementation (baseline parameters)\n",
    "num_folds = 10\n",
    "scoring_metrics = ['accuracy', 'precision', 'recall']\n",
    "print(f'Performing cross-validation with {num_folds} folds...')\n",
    "scores_manual_baseline = cross_validate_manual(mlp, x_train, y_train, cv=num_folds, scoring=scoring_metrics)\n",
    "\n",
    "print(f'\\n{num_folds}-fold validation accuracy mean: {round(scores_manual_baseline[\"test_accuracy\"].mean(), 3) * 100}%')\n",
    "print(f'{num_folds}-fold validation precision mean: {round(scores_manual_baseline[\"test_precision\"].mean(), 3) * 100}%')\n",
    "print(f'{num_folds}-fold validation recall mean: {round(scores_manual_baseline[\"test_recall\"].mean(), 3) * 100}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find p-values for accuracy, precision, and recall from cross-validations.\n",
    "\n",
    "Got help from Ashley for this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy p-val: 5.1066823670443025e-09\n",
      "Precision p-val: 5.1066823670443025e-09\n",
      "Recall p-val: 5.1066823670443025e-09\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.05  # Significance level\n",
    "\n",
    "ttest_accuracy, p_val_accuracy = ttest_rel(scores_modified['test_accuracy'], scores_manual_baseline['test_accuracy'])\n",
    "ttest_precision, p_val_precision = ttest_rel(\n",
    "    scores_modified['test_precision'], scores_manual_baseline['test_precision']\n",
    ")\n",
    "ttest_recall, p_val_recall = ttest_rel(scores_modified['test_recall'], scores_manual_baseline['test_recall'])\n",
    "\n",
    "print(f'\\nAccuracy p-val: {p_val_accuracy}')\n",
    "print(f'Precision p-val: {p_val_accuracy}')\n",
    "print(f'Recall p-val: {p_val_accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now repeat the same experiment, but with different parameters that make my manual implementation perform better:\n",
    "\n",
    "- `max-iter` is 10000. My epochs don't seem to take as long as sklearn's, and more iterations greatly help my model.\n",
    "\n",
    "- `learning_rate` is `0.1` to help my manual implementation find a local minimum more quickly during stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training neural network...\n",
      "Epoch: 0; Loss: 1.6094240230069676\n",
      "Epoch: 2000; Loss: 1.463788552663659\n",
      "Epoch: 4000; Loss: 1.381777374263621\n",
      "Epoch: 6000; Loss: 1.3473923658904607\n",
      "Epoch: 8000; Loss: 1.3305103163863892\n",
      "\n",
      "Accuracy on training: 42.0%\n",
      "Accuracy on validation: 37.1%\n",
      "Accuracy on test: 39.300000000000004%\n",
      "\n",
      "Average weighted precision: 38.6%\n",
      "Average weighted recall: 39.300000000000004%\n",
      "\n",
      "Class 0: Precision = 0.0%, Recall = 0.0%\n",
      "Class 1: Precision = 33.900000000000006%, Recall = 39.300000000000004%\n",
      "Class 2: Precision = 33.5%, Recall = 47.4%\n",
      "Class 3: Precision = 38.2%, Recall = 13.0%\n",
      "Class 4: Precision = 51.2%, Recall = 55.900000000000006%\n",
      "\n",
      "Training predictions: [4 4 3 ... 1 4 3]\n",
      "Counts of each class in prediction: {1: 1743, 2: 3414, 3: 1187, 4: 6232}\n",
      "Test predictions: [4 1 2 ... 1 4 1]\n",
      "Counts of each class in prediction: {1: 354, 2: 565, 3: 110, 4: 451}\n"
     ]
    }
   ],
   "source": [
    "mlp = NeuralNet(\n",
    "    input_num_features=37, output_num_classes=5, hidden_layer_sizes=(37, 5), max_iter=10000, learning_rate=0.1\n",
    ")\n",
    "\n",
    "# Fit to training data and make predictions on test data\n",
    "print('Training neural network...')\n",
    "train_predictions = mlp.fit(x_train, y_train)\n",
    "val_predictions = mlp.predict(x_val)\n",
    "test_predictions = mlp.predict(x_test)\n",
    "\n",
    "train_accuracy = accuracy_score(train_predictions, y_train.to_numpy())\n",
    "print(f'\\nAccuracy on training: {round(train_accuracy, 3) * 100}%')\n",
    "\n",
    "val_accuracy = accuracy_score(val_predictions, y_val.to_numpy())\n",
    "print(f'Accuracy on validation: {round(accuracy, 3) * 100}%')\n",
    "\n",
    "test_accuracy = accuracy_score(test_predictions, y_test.to_numpy())\n",
    "print(f'Accuracy on test: {round(test_accuracy, 3) * 100}%\\n')\n",
    "\n",
    "# zero_divison=0 returns 0 precision/recall if no positive samples for a class\n",
    "avg_precision = precision_score(y_test, test_predictions, average='weighted', zero_division=0)\n",
    "avg_recall = recall_score(y_test, test_predictions, average='weighted', zero_division=0)\n",
    "precision_per_class = precision_score(y_test, test_predictions, average=None, zero_division=0)\n",
    "recall_per_class = recall_score(y_test, test_predictions, average=None, zero_division=0)\n",
    "\n",
    "print(f'Average weighted precision: {round(avg_precision, 3) * 100}%')\n",
    "print(f'Average weighted recall: {round(avg_recall, 3) * 100}%\\n')\n",
    "for class_label, precision, recall in zip(range(len(precision_per_class)), precision_per_class, recall_per_class):\n",
    "    print(f'Class {class_label}: Precision = {round(precision, 3) * 100}%, Recall = {round(recall, 3) * 100}%')\n",
    "print()\n",
    "\n",
    "unique_values, counts = np.unique(train_predictions, return_counts=True)\n",
    "print(f'Training predictions: {train_predictions}')\n",
    "print(f'Counts of each class in prediction: {dict(zip(unique_values, counts))}')\n",
    "\n",
    "unique_values, counts = np.unique(test_predictions, return_counts=True)\n",
    "print(f'Test predictions: {test_predictions}')\n",
    "print(f'Counts of each class in prediction: {dict(zip(unique_values, counts))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform k-fold cross validation for manual implementation with modified parameters.\n",
    "\n",
    "Output cross-validation accuracy, precision, and recall (weighted averages)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing cross-validation with 10 folds...\n",
      "Epoch: 0; Loss: 1.3124350664169302\n",
      "Epoch: 2000; Loss: 1.296672883967074\n",
      "Epoch: 4000; Loss: 1.2894313093995198\n",
      "Epoch: 6000; Loss: 1.2759197359298349\n",
      "Epoch: 8000; Loss: 1.2794160173653162\n",
      "Epoch: 0; Loss: 1.2748584567959464\n",
      "Epoch: 2000; Loss: 1.2578926791192646\n",
      "Epoch: 4000; Loss: 1.2608271645901752\n",
      "Epoch: 6000; Loss: 1.2541892221192297\n",
      "Epoch: 8000; Loss: 1.2531472943839617\n",
      "Epoch: 0; Loss: 1.2555860405503578\n",
      "Epoch: 2000; Loss: 1.2605270730823566\n",
      "Epoch: 4000; Loss: 1.2382423449477447\n",
      "Epoch: 6000; Loss: 1.2391154465942367\n",
      "Epoch: 8000; Loss: 1.2370342723064258\n",
      "Epoch: 0; Loss: 1.2483579734204224\n",
      "Epoch: 2000; Loss: 1.2348287560931186\n",
      "Epoch: 4000; Loss: 1.2349045304484019\n",
      "Epoch: 6000; Loss: 1.2278369306404964\n",
      "Epoch: 8000; Loss: 1.2391186450808014\n",
      "Epoch: 0; Loss: 1.236017399318491\n",
      "Epoch: 2000; Loss: 1.2230810522429787\n",
      "Epoch: 4000; Loss: 1.2201077796848903\n",
      "Epoch: 6000; Loss: 1.2202236330842509\n",
      "Epoch: 8000; Loss: 1.2183523318528693\n",
      "Epoch: 0; Loss: 1.231835047513084\n",
      "Epoch: 2000; Loss: 1.215765638640833\n",
      "Epoch: 4000; Loss: 1.2189587802038193\n",
      "Epoch: 6000; Loss: 1.2181639341748467\n",
      "Epoch: 8000; Loss: 1.2192198704361703\n",
      "Epoch: 0; Loss: 1.2335182078309535\n",
      "Epoch: 2000; Loss: 1.2204631425340864\n",
      "Epoch: 4000; Loss: 1.2167135081744782\n",
      "Epoch: 6000; Loss: 1.2140569752375667\n",
      "Epoch: 8000; Loss: 1.2162265834461905\n",
      "Epoch: 0; Loss: 1.2260687183102978\n",
      "Epoch: 2000; Loss: 1.2126968205910347\n",
      "Epoch: 4000; Loss: 1.2079688587729474\n",
      "Epoch: 6000; Loss: 1.2127858626411652\n",
      "Epoch: 8000; Loss: 1.2078013350951997\n",
      "Epoch: 0; Loss: 1.2254868253224678\n",
      "Epoch: 2000; Loss: 1.2089961487275245\n",
      "Epoch: 4000; Loss: 1.208923952321845\n",
      "Epoch: 6000; Loss: 1.2058999110976136\n",
      "Epoch: 8000; Loss: 1.210775176516002\n",
      "Epoch: 0; Loss: 1.217567331455223\n",
      "Epoch: 2000; Loss: 1.2122492714349786\n",
      "Epoch: 4000; Loss: 1.2054738106394836\n",
      "Epoch: 6000; Loss: 1.206973006852917\n",
      "Epoch: 8000; Loss: 1.2041074052804128\n",
      "\n",
      "10-fold validation accuracy mean: 41.8%\n",
      "10-fold validation precision mean: 39.7%\n",
      "10-fold validation recall mean: 41.8%\n"
     ]
    }
   ],
   "source": [
    "# Get cross-validation metrics for the manual implementation (modified parameters)\n",
    "num_folds = 10\n",
    "scoring_metrics = ['accuracy', 'precision', 'recall']\n",
    "print(f'Performing cross-validation with {num_folds} folds...')\n",
    "scores_manual_modified = cross_validate_manual(mlp, x_train, y_train, cv=num_folds, scoring=scoring_metrics)\n",
    "\n",
    "print(f'\\n{num_folds}-fold validation accuracy mean: {round(scores_manual_modified[\"test_accuracy\"].mean(), 3) * 100}%')\n",
    "print(f'{num_folds}-fold validation precision mean: {round(scores_manual_modified[\"test_precision\"].mean(), 3) * 100}%')\n",
    "print(f'{num_folds}-fold validation recall mean: {round(scores_manual_modified[\"test_recall\"].mean(), 3) * 100}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform k-fold cross validation for the modified sklearn implementation.\n",
    "\n",
    "Output cross-validation accuracy, precision, and recall (weighted averages)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing cross-validation with 10 folds...\n",
      "\n",
      "10-fold validation accuracy mean: 37.1%\n",
      "10-fold validation precision mean: 35.6%\n",
      "10-fold validation recall mean: 37.1%\n"
     ]
    }
   ],
   "source": [
    "# Get cross-validation metrics for \"sklearn Modified\" using our function\n",
    "classifier = MLPClassifier(max_iter=500, hidden_layer_sizes=(38, 5), random_state=42)\n",
    "num_folds = 10\n",
    "scoring_metrics = ['accuracy', 'precision', 'recall']\n",
    "print(f'Performing cross-validation with {num_folds} folds...')\n",
    "scores_modified_custom = cross_validate_manual(classifier, x_train, y_train, cv=num_folds, scoring=scoring_metrics)\n",
    "\n",
    "print(f'\\n{num_folds}-fold validation accuracy mean: {round(scores_modified_custom[\"test_accuracy\"].mean(), 3) * 100}%')\n",
    "print(f'{num_folds}-fold validation precision mean: {round(scores_modified_custom[\"test_precision\"].mean(), 3) * 100}%')\n",
    "print(f'{num_folds}-fold validation recall mean: {round(scores_modified_custom[\"test_recall\"].mean(), 3) * 100}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find p-values for accuracy, precision, and recall from cross-validations.\n",
    "\n",
    "Got help from Ashley for this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy p-val: 0.00010913491703793282\n",
      "Precision p-val: 0.00010913491703793282\n",
      "Recall p-val: 0.00010913491703793282\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.05  # Significance level\n",
    "\n",
    "ttest_accuracy, p_val_accuracy = ttest_rel(\n",
    "    scores_manual_modified['test_accuracy'], scores_modified_custom['test_accuracy']\n",
    ")\n",
    "ttest_precision, p_val_precision = ttest_rel(\n",
    "    scores_manual_modified['test_precision'], scores_modified_custom['test_precision']\n",
    ")\n",
    "ttest_recall, p_val_recall = ttest_rel(scores_manual_modified['test_recall'], scores_modified_custom['test_recall'])\n",
    "\n",
    "print(f'\\nAccuracy p-val: {p_val_accuracy}')\n",
    "print(f'Precision p-val: {p_val_accuracy}')\n",
    "print(f'Recall p-val: {p_val_accuracy}')\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
