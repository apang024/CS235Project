{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Pre-Processing"
      ],
      "metadata": {
        "id": "OAEnnMAIwIX8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Citations:\n",
        "- [TA provided sample project](https://elearn.ucr.edu/courses/104198/pages/sample-project-material?module_item_id=1752456)\n",
        "- [Google Drive Mounting and Folder Path](https://stackoverflow.com/questions/72199130/google-colab-import-data-from-google-drive-and-make-it-possible-to-share-it)\n",
        "- [SKLearn Libraries for Decision Trees](https://scikit-learn.org/stable/modules/tree.html)\n",
        "- Preprocessing/Metric Representation - similar to the team\n",
        "---\n",
        "\n",
        "**Goal:** Predict adoption speed of animal based on features given by adoption centers to improve allocation of resources for long-term animals.\n",
        "\n",
        "---\n",
        "Here is a key for the given attributes:\n",
        "- adoption speed (value to predict) (0 - same day, 1 - one and seven days, 2 - eight and thirty days, 3 - thirty-one and ninety days, 4  - no adoption after 100 days)\n",
        "- animal type (1 = dog/2 = cat)\n",
        "- age (in months)\n",
        "- breed (refer to labels)\n",
        "- gender (1 = Male, 2 = Female, 3 = Multiple in one Post)\n",
        "- color (refer to labels)\n",
        "- maturity (1 = Small, 2 = Medium, 3 = Large, 4 = Extra Large, 0 = Not Specified)\n",
        "- fur length  (1 = Short, 2 = Medium, 3 = Long, 0 = Not Specified)\n",
        "- vaccination status  (1 = Yes, 2 = No, 3 = Not Sure)\n",
        "- dewormed status  (1 = Yes, 2 = No, 3 = Not Sure)\n",
        "- sterilization  (1 = Yes, 2 = No, 3 = Not Sure)\n",
        "- health condition  (1 = Healthy, 2 = Minor Injury, 3 = Serious Injury, 0 = Not Specified)\n",
        "- quantity (number of pets in listing)\n",
        "- adoption fee (in RM)\n",
        "- location (state in Malaysia, refer to labels)\n",
        "---\n"
      ],
      "metadata": {
        "id": "2bKLnG01f9uN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup Libraries\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import plot_tree\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "from sklearn.feature_extraction import FeatureHasher\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.metrics import make_scorer\n",
        "from scipy import stats\n",
        "\n",
        "# Setup Dataframes\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import train_test_split\n",
        "drive.mount('/content/drive/', force_remount=True)\n",
        "df_train = pd.read_csv( \"/content/drive/My Drive/Colab Notebooks/train.csv\")"
      ],
      "metadata": {
        "id": "clifCREyaDVS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f13fec50-ace1-4790-8d3a-eea8620cd69b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop columns\n",
        "df_train = df_train.drop(['Name', 'RescuerID', 'PetID', 'Description'], axis=1)\n",
        "\n",
        "# Drop rows with missing information\n",
        "df_train = df_train.dropna()\n",
        "\n",
        "# Calculate z-score for only numerical columns\n",
        "z_scores_fee = np.abs(stats.zscore(df_train['Fee']))\n",
        "z_scores_age = np.abs(stats.zscore(df_train['Age']))\n",
        "\n",
        "# Using a threshold of 5 std dev. identify the outlier rows\n",
        "outlier_rows_fee = z_scores_fee > 5\n",
        "outlier_rows_age = z_scores_age > 5\n",
        "combined_outlier_rows = outlier_rows_fee | outlier_rows_age\n",
        "\n",
        "# Remove rows with outliers\n",
        "df_train = df_train[~combined_outlier_rows]\n",
        "print(f\"Removed {combined_outlier_rows.sum()} rows due to outliers. There are now {df_train.shape[0]} rows in our unsplit training data.\")\n",
        "\n",
        "# Separate the resulting column from the training data\n",
        "adoption_speed_train = df_train.pop('AdoptionSpeed')"
      ],
      "metadata": {
        "id": "V09E2JOReEOp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59b295ea-a171-4cda-9389-2b09bbb3067d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removed 197 rows due to outliers. There are now 14796 rows in our unsplit training data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For categorical features, we have to encode them in order to avoid having the number associated with the feature implying an order. I used separated them into binary, ordinal, and non-ordinal categories.\n",
        "\n",
        "- Gender, etc. are binary as there are only 3 options for each (ex: Yes, No, Not sure) and we can easily use one-hot encoding through pandas' get_dummies function\n",
        "- Maturity, Size, Color, etc. have an order to them (ex: 1 = black, 7 = white), and so color is ordered from dark to light which means that there is a natural order and we can use Label encoding\n",
        "- For Breed since it is non-ordinal, we will have to split Breed into different features, and we chose the top 8 breeds and calculated their frequencies for each column, and created another column for all other."
      ],
      "metadata": {
        "id": "VtGEmvA3PKPq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separating categorical features\n",
        "binary_categorical_columns = ['Gender','Vaccinated','Dewormed', 'Sterilized']\n",
        "ordinal_categorical_columns = ['MaturitySize', 'FurLength', 'Health', 'Color1', 'Color2', 'Color3']\n",
        "nonordinal_categorical_columns = ['Breed1', 'Breed2']\n",
        "\n",
        "# Encoding\n",
        "label_encoder = LabelEncoder()\n",
        "df_train = pd.get_dummies(df_train, columns=binary_categorical_columns)\n",
        "for column in ordinal_categorical_columns:\n",
        "  df_train[column] = label_encoder.fit_transform(df_train[column])\n",
        "\n",
        "# Split non-ordinal categorical columns into n features (similar to Benjamin's preprocessing)\n",
        "n = 3\n",
        "\n",
        "for feature in nonordinal_categorical_columns:\n",
        "  top_N_values = df_train[feature].value_counts().head(n)\n",
        "  print(f'Top {n} values for {feature}:\\n{top_N_values}\\n')\n",
        "\n",
        "  top_N_value_names = top_N_values.index\n",
        "  for index, row in df_train.iterrows():\n",
        "    # If value isn't top N frequency, replace with -1 (other)\n",
        "    if row[feature] not in top_N_value_names:\n",
        "      df_train.at[index, feature] = -1\n",
        "\n",
        "  df_train = pd.get_dummies(df_train, columns=[feature])"
      ],
      "metadata": {
        "id": "XsE0ifAMFmbz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f35de7e3-8a75-441b-b7dd-2042928979ce"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 3 values for Breed1:\n",
            "307    5903\n",
            "266    3623\n",
            "265    1257\n",
            "Name: Breed1, dtype: int64\n",
            "\n",
            "Top 3 values for Breed2:\n",
            "0      10613\n",
            "307     1723\n",
            "266      597\n",
            "Name: Breed2, dtype: int64\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split 85% training, 10% test, 5% validation\n",
        "df_train_85, X_temp, adoption_speed_train_85, y_temp = train_test_split(df_train, adoption_speed_train, test_size=0.15, random_state=42, stratify=adoption_speed_train)    # 85% training set, 15% temp set\n",
        "df_test_10, df_val_5, adoption_speed_test_10, adoption_speed_val_5 = train_test_split(X_temp, y_temp, test_size=1/3, random_state=42, stratify=y_temp)       # 10% test set, 5% validation set"
      ],
      "metadata": {
        "id": "H3OtuoDbxaUC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Out of curiostiy, I also plot the pairwise feature plot for any values that are features with continuous values to view their corresponding plots. I can see from these graphs that many of the graphs are right skewed.\n",
        "\n",
        "I did not plot any other features as their values are categorical and do not give very interesting graphs as many of them group vertically due to their categorical nature."
      ],
      "metadata": {
        "id": "e8b9_GtAfKzf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.pairplot(df_train_85[['Age', 'Quantity', 'Fee']], diag_kind=\"hist\")"
      ],
      "metadata": {
        "id": "D7liTifQt7_A"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the decision tree model:\n",
        "- max_depth: helps to restrict the max depth that the decision tree can be (decrease to help with overfitting)\n",
        "- min_samples_split: the smallest number that an internal node needs to split into child nodes (increase to help with overfitting)\n",
        "- min_samples_leaf: the smallest number that an external node needs to split into child nodes (increase to help with overfitting)"
      ],
      "metadata": {
        "id": "0OztF0H0hfln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a decision tree model (default)\n",
        "decision_tree_default = DecisionTreeClassifier()\n",
        "decision_tree_default.fit(df_train_85, adoption_speed_train_85)\n",
        "adoption_speed_train_prediction_default = decision_tree_default.predict(df_train_85)\n",
        "adoption_speed_val_prediction_default = decision_tree_default.predict(df_val_5)\n",
        "adoption_speed_test_prediction_default = decision_tree_default.predict(df_test_10)\n",
        "\n",
        "\n",
        "# Create a decision tree model (pre-pruning)\n",
        "decision_tree = DecisionTreeClassifier(max_depth=10, min_samples_split=2, min_samples_leaf=2)\n",
        "decision_tree.fit(df_train_85, adoption_speed_train_85)\n",
        "adoption_speed_train_prediction = decision_tree.predict(df_train_85)\n",
        "adoption_speed_val_prediction = decision_tree.predict(df_val_5)\n",
        "adoption_speed_test_prediction = decision_tree.predict(df_test_10)"
      ],
      "metadata": {
        "id": "9iLjTAzq1FpT"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# OFF THE SHELF BASELINE MODEL\n",
        "print(f\"Before Pre-Pruning (default baseline model):\")\n",
        "\n",
        "datasets = [\n",
        "    (adoption_speed_train_85, adoption_speed_train_prediction_default, 'Training'),\n",
        "    (adoption_speed_val_5, adoption_speed_val_prediction_default, 'Validation'),\n",
        "    (adoption_speed_test_10, adoption_speed_test_prediction_default, 'Test')\n",
        "]\n",
        "\n",
        "for data, predictions, label in datasets:\n",
        "  # Calculate and print accuracy, precion, and recall per class (FOR TRAINING)\n",
        "  accuracy = accuracy_score(data, predictions)\n",
        "  print(f\"\\nAccuracy ({label}): {accuracy}\")\n",
        "  # Calculate and print average weighted precision and recall for classes overall\n",
        "  avg_precision = precision_score(data, predictions, average='weighted', zero_division=0)\n",
        "  avg_recall = recall_score(data, predictions, average='weighted', zero_division=0)\n",
        "  print(f'Average weighted precision: {avg_precision}')\n",
        "  print(f'Average weighted recall: {avg_recall}')\n",
        "  # Calculate precision and recall for each class separately with zero_division = 0 so that precision or recall as 0.0 for classes with no predicted samples\n",
        "  precision_per_class = precision_score(data, predictions, average=None, zero_division=0)\n",
        "  recall_per_class = recall_score(data, predictions, average=None, zero_division=0)\n",
        "  # Print precision and recall for each class\n",
        "  for class_label, precision, recall in zip(range(len(precision_per_class)), precision_per_class, recall_per_class):\n",
        "      print(f'Class {class_label}: Precision = {precision:.2f}, Recall = {recall:.2f}')"
      ],
      "metadata": {
        "id": "LN3WpG6m61Z8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65c59dc4-d3e0-4432-de94-73d551d2b1ad"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before Pre-Pruning (default baseline model):\n",
            "\n",
            "Accuracy (Training): 0.9838581424936387\n",
            "Average weighted precision: 0.9840000811568964\n",
            "Average weighted recall: 0.9838581424936387\n",
            "Class 0: Precision = 0.98, Recall = 0.99\n",
            "Class 1: Precision = 0.97, Recall = 0.99\n",
            "Class 2: Precision = 0.98, Recall = 0.98\n",
            "Class 3: Precision = 0.99, Recall = 0.98\n",
            "Class 4: Precision = 1.00, Recall = 0.98\n",
            "\n",
            "Accuracy (Validation): 0.3364864864864865\n",
            "Average weighted precision: 0.3345237353546888\n",
            "Average weighted recall: 0.3364864864864865\n",
            "Class 0: Precision = 0.12, Recall = 0.15\n",
            "Class 1: Precision = 0.29, Recall = 0.29\n",
            "Class 2: Precision = 0.31, Recall = 0.28\n",
            "Class 3: Precision = 0.27, Recall = 0.27\n",
            "Class 4: Precision = 0.46, Recall = 0.50\n",
            "\n",
            "Accuracy (Test): 0.3290540540540541\n",
            "Average weighted precision: 0.33730794223059996\n",
            "Average weighted recall: 0.3290540540540541\n",
            "Class 0: Precision = 0.06, Recall = 0.10\n",
            "Class 1: Precision = 0.27, Recall = 0.28\n",
            "Class 2: Precision = 0.35, Recall = 0.34\n",
            "Class 3: Precision = 0.30, Recall = 0.32\n",
            "Class 4: Precision = 0.43, Recall = 0.39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# USING ADJUSTED MODEL PARAMETERS (Pre-pruning)\n",
        "print(f\"After Pre-Pruning:\")\n",
        "datasets = [\n",
        "    (adoption_speed_train_85, adoption_speed_train_prediction, 'Training'),\n",
        "    (adoption_speed_val_5, adoption_speed_val_prediction, 'Validation'),\n",
        "    (adoption_speed_test_10, adoption_speed_test_prediction, 'Test')\n",
        "]\n",
        "\n",
        "for data, predictions, label in datasets:\n",
        "  # Calculate and print accuracy, precion, and recall per class (FOR TRAINING)\n",
        "  accuracy = accuracy_score(data, predictions)\n",
        "  print(f\"\\nAccuracy ({label}): {accuracy}\")\n",
        "  # Calculate and print average weighted precision and recall for classes overall\n",
        "  avg_precision = precision_score(data, predictions, average='weighted', zero_division=0)\n",
        "  avg_recall = recall_score(data, predictions, average='weighted', zero_division=0)\n",
        "  print(f'Average weighted precision: {avg_precision}')\n",
        "  print(f'Average weighted recall: {avg_recall}')\n",
        "  # Calculate precision and recall for each class separately with zero_division = 0 so that precision or recall as 0.0 for classes with no predicted samples\n",
        "  precision_per_class = precision_score(data, predictions, average=None, zero_division=0)\n",
        "  recall_per_class = recall_score(data, predictions, average=None, zero_division=0)\n",
        "  # Print precision and recall for each class\n",
        "  for class_label, precision, recall in zip(range(len(precision_per_class)), precision_per_class, recall_per_class):\n",
        "      print(f'Class {class_label}: Precision = {precision:.2f}, Recall = {recall:.2f}')\n"
      ],
      "metadata": {
        "id": "wXoNzAtKiWg6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5aaae8ba-64db-4db2-e6fe-d59536ffca0d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After Pre-Pruning:\n",
            "\n",
            "Accuracy (Training): 0.47892811704834604\n",
            "Average weighted precision: 0.4807584954752817\n",
            "Average weighted recall: 0.47892811704834604\n",
            "Class 0: Precision = 0.47, Recall = 0.08\n",
            "Class 1: Precision = 0.42, Recall = 0.43\n",
            "Class 2: Precision = 0.42, Recall = 0.50\n",
            "Class 3: Precision = 0.48, Recall = 0.36\n",
            "Class 4: Precision = 0.59, Recall = 0.62\n",
            "\n",
            "Accuracy (Validation): 0.3878378378378378\n",
            "Average weighted precision: 0.3776627687053279\n",
            "Average weighted recall: 0.3878378378378378\n",
            "Class 0: Precision = 0.00, Recall = 0.00\n",
            "Class 1: Precision = 0.33, Recall = 0.35\n",
            "Class 2: Precision = 0.34, Recall = 0.40\n",
            "Class 3: Precision = 0.33, Recall = 0.27\n",
            "Class 4: Precision = 0.53, Recall = 0.54\n",
            "\n",
            "Accuracy (Test): 0.3587837837837838\n",
            "Average weighted precision: 0.3576067787118627\n",
            "Average weighted recall: 0.3587837837837838\n",
            "Class 0: Precision = 0.25, Recall = 0.03\n",
            "Class 1: Precision = 0.30, Recall = 0.32\n",
            "Class 2: Precision = 0.31, Recall = 0.39\n",
            "Class 3: Precision = 0.31, Recall = 0.22\n",
            "Class 4: Precision = 0.49, Recall = 0.49\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Cross Valdiation on default and pre-pruned decision trees (Worked with Ben)\n",
        "scoring_metrics = {\n",
        "    'accuracy': make_scorer(accuracy_score),\n",
        "    'precision': make_scorer(precision_score, average='weighted', zero_division=0),\n",
        "    'recall': make_scorer(recall_score, average='weighted', zero_division=0)\n",
        "}\n",
        "\n",
        "scores_default = cross_validate(decision_tree_default, df_train_85, adoption_speed_train_85, cv=10, scoring=scoring_metrics)\n",
        "print(f'10-fold validation accuracy mean: {scores_default[\"test_accuracy\"].mean()}')\n",
        "print(f'10-fold validation precision mean: {scores_default[\"test_precision\"].mean()}')\n",
        "print(f'10-fold validation recall mean: {scores_default[\"test_recall\"].mean()}')\n",
        "\n",
        "scores_pruned = cross_validate(decision_tree, df_train_85, adoption_speed_train_85, cv=10, scoring=scoring_metrics)\n",
        "print(f'\\n10-fold validation accuracy mean: {scores_pruned[\"test_accuracy\"].mean()}')\n",
        "print(f'10-fold validation precision mean: {scores_pruned[\"test_precision\"].mean()}')\n",
        "print(f'10-fold validation recall mean: {scores_pruned[\"test_recall\"].mean()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWh0GK1Lin0i",
        "outputId": "95fe3b16-27b4-43c9-9aeb-c2f98abd45d3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-fold validation accuracy mean: 0.32394868545366934\n",
            "10-fold validation precision mean: 0.32500862490708754\n",
            "10-fold validation recall mean: 0.32394868545366934\n",
            "\n",
            "10-fold validation accuracy mean: 0.3663309315211603\n",
            "10-fold validation precision mean: 0.35646408440380384\n",
            "10-fold validation recall mean: 0.3663309315211603\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform T-test for statistical significance\n",
        "t_stat_accuracy, p_value_accuracy = stats.ttest_rel(scores_default[\"test_accuracy\"], scores_pruned[\"test_accuracy\"])\n",
        "t_stat_precision, p_value_precision = stats.ttest_rel(scores_default[\"test_precision\"], scores_pruned[\"test_precision\"])\n",
        "t_stat_recall, p_value_recall = stats.ttest_rel(scores_default[\"test_recall\"], scores_pruned[\"test_recall\"])\n",
        "\n",
        "print(f'p_value_accuracy = {p_value_accuracy}')\n",
        "print(f'p_value_precision = {p_value_precision}')\n",
        "print(f'p_value_recall = {p_value_recall}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xxd3tvDHm8p-",
        "outputId": "b998d666-a099-4986-f2f8-a6cca07ab34c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "p_value_accuracy = 9.099466736064099e-07\n",
            "p_value_precision = 1.3764931213505677e-05\n",
            "p_value_recall = 9.099466736064099e-07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import export_text\n",
        "r = export_text(decision_tree, feature_names=df_train_85.columns.tolist())\n",
        "print(r)"
      ],
      "metadata": {
        "id": "33fhILuc4cCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_tree(decision_tree)"
      ],
      "metadata": {
        "id": "ZJeTv56isO6X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}